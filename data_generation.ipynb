{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import spike_queries\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel, AutoTokenizer, AutoModel, PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Tuple\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries2.txt\", \"r\") as f:\n",
    "    queries = f.readlines()\n",
    "    queries = [l.strip() for l in queries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,q in enumerate(queries):\n",
    "    try:\n",
    "        assert len(q.split(\"\\t\")) == 2\n",
    "    except:\n",
    "        print(i, q, len(q.split(\"\\t\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,q in enumerate(queries):\n",
    "    a = q.count(\"arg1:\") == q.count(\"arg2:\") == 1\n",
    "    if not a:\n",
    "        print(i,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<>arg1:virus $infection $causes a <>arg2:condition', '<>arg1:virus $infection, which $causes a <>arg2:condition', '<>arg1:virus $infection $results $in <>arg2:condition', '<>arg1:virus $infection, which $results $in <>arg2:condition', '<>arg2:condition $is $the $result of <>arg1:virus $infection', '<>arg2:condition, which $is $the $result of <>arg1:virus $infection', '<>arg2:condition $is $[l=cause|trigger]caused $by <>arg1:virus $infection', 'as a result of <>arg1:virus $infection, <>arg2:condition $can $develop']\n"
     ]
    }
   ],
   "source": [
    "queries2results = defaultdict(list)\n",
    "id2query = defaultdict(list)\n",
    "\n",
    "for id_and_query in queries:\n",
    "    query,id = id_and_query.split(\"\\t\")\n",
    "    id2query[int(id)].append(query)\n",
    "    \n",
    "print(id2query[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"covid19\"\n",
    "# num_results = 100\n",
    "# query_type = \"syntactic\"\n",
    "\n",
    "# for i, q_and_id in tqdm.tqdm(enumerate(queries), total = len(queries)):\n",
    "#     q, id = q_and_id.split(\"\\t\")\n",
    "#     id = int(id)\n",
    "#     try:\n",
    "#         df = spike_queries.perform_query(q, dataset_name, num_results, query_type) #previously: word=Hawaii\n",
    "#         queries2results[id].append(df)\n",
    "#         time.sleep(2)\n",
    "#     except Exception as e:\n",
    "#         print(\"Error\", i+1)\n",
    "#         print(e)\n",
    "        \n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, dfs in queries2results.items():\n",
    "#      for i,df in enumerate(dfs):\n",
    "#          print(id, i)\n",
    "#          queries2results[id][i] = df.dropna(subset=['arg1_first_index',\"arg1_last_index\",\"arg2_first_index\",\"arg2_last_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, dfs in queries2results.items():\n",
    "      for i,df in enumerate(dfs):\n",
    "            queries2results[id][i][\"spike_query\"] = id2query[id][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"queries2results.pickle\", \"wb\") as f:\n",
    "#       pickle.dump(queries2results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries2results.pickle\", \"rb\") as f:\n",
    "    queries2results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries2results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries2results_new = dict()\n",
    "# for k,dfs in queries2results.items():\n",
    "#     if k == 0: continue\n",
    "#     queries2results_new[k] = pd.concat(dfs, axis=0)\n",
    "    \n",
    "# queries2results = queries2results_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = set([int(l.split(\"\\t\")[-1]) for l in queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments(sent:str, arg1_start, arg1_end, arg2_start, arg2_end):\n",
    "    \n",
    "        s_lst = sent.split(\" \")\n",
    "        if arg1_start > arg2_start:\n",
    "            arg1_start, arg2_start = arg2_start, arg1_start\n",
    "            arg1_end, arg2_end = arg2_end, arg1_end\n",
    "            arg1_str, arg2_str = \"<<ARG2:\", \"<<ARG1:\"\n",
    "        else:\n",
    "            arg1_str, arg2_str = \"<<ARG1:\", \"<<ARG2:\"\n",
    "        \n",
    "        s_with_args = s_lst[:arg1_start] + [arg1_str] + s_lst[arg1_start:arg1_end+1] + [\">>\"] + s_lst[arg1_end+1:arg2_start] + [arg2_str] + s_lst[arg2_start:arg2_end+1] + [\">>\"] +s_lst[arg2_end+1:]  \n",
    "        #s_with_args = s_lst[:arg1_start] + [arg1_str+s_lst[arg1_ind]] + s_lst[arg1_ind+1:arg2_ind] + [arg2_str+s_lst[arg2_ind]] + s_lst[arg2_ind+1:]\n",
    "        s_with_args = \" \".join(s_with_args).replace(\"ARG1: \", \"ARG1:\").replace(\"ARG2: \", \"ARG2:\")\n",
    "        s_with_args = s_with_args.replace(\" >>\", \">>\")\n",
    "        return s_with_args\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:09<00:00,  5.75it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_PAIRS_PER_RELATION = 500\n",
    "\n",
    "data = []\n",
    "\n",
    "for id in tqdm.tqdm(ids): #foreach relation\n",
    "    \n",
    "    pattern_pairs = list(itertools.product(range(len(id2query[id])), repeat=2))\n",
    "    pattern_pairs_different = [(p1,p2) for (p1,p2) in pattern_pairs if p1 != p2]\n",
    "    \n",
    "    sum_pattern = 0\n",
    "    relation_examples = []\n",
    "    \n",
    "    for p1, p2 in pattern_pairs_different:\n",
    "        df1,df2 = queries2results[id][p1], queries2results[id][p2]\n",
    "        if df1.empty or df2.empty: continue\n",
    "            \n",
    "        sentences1, sentences2 = df1[\"sentence_text\"].tolist(), df2[\"sentence_text\"].tolist()\n",
    "        query1, query2 = df1[\"spike_query\"].tolist()[0], df2[\"spike_query\"].tolist()[0]\n",
    "        arg1_first_start, arg1_first_end = df1[\"arg1_first_index\"].tolist(), df1[\"arg1_last_index\"].tolist()\n",
    "        arg2_first_start, arg2_first_end = df1[\"arg2_first_index\"].tolist(), df1[\"arg2_last_index\"].tolist()\n",
    "        arg1_second_start, arg1_second_end = df2[\"arg1_first_index\"].tolist(), df2[\"arg1_last_index\"].tolist()\n",
    "        arg2_second_start, arg2_second_end = df2[\"arg2_first_index\"].tolist(), df2[\"arg2_last_index\"].tolist()\n",
    "        \n",
    "        \n",
    "        all_pair_combinations = list(itertools.product(range(len(df1)), range(len(df2))))\n",
    "        random.shuffle(all_pair_combinations)\n",
    "        for combination in all_pair_combinations[:100]:\n",
    "            ind1, ind2 = combination\n",
    "            \n",
    "            sent1, sent2 = sentences1[ind1], sentences2[ind2]\n",
    "            sent1_arg1_start, sent1_arg1_end = arg1_first_start[ind1], arg1_first_end[ind1]\n",
    "            sent1_arg2_start, sent1_arg2_end = arg2_first_start[ind1], arg2_first_end[ind1]\n",
    "            sent2_arg1_start, sent2_arg1_end = arg1_second_start[ind2], arg1_second_end[ind2]\n",
    "            sent2_arg2_start, sent2_arg2_end = arg2_second_start[ind2], arg2_second_end[ind2]\n",
    "            sent1_with_args = add_arguments(sent1, sent1_arg1_start, sent1_arg1_end, sent1_arg2_start, sent1_arg2_end)\n",
    "            sent2_with_args = add_arguments(sent2, sent2_arg1_start, sent2_arg1_end, sent2_arg2_start, sent2_arg2_end)\n",
    "            \n",
    "            sent1_lst = sent1.split(\" \")\n",
    "            sent2_lst = sent2.split(\" \")\n",
    "            \n",
    "            sent1_arg1_w = sent1_lst[sent1_arg1_start:sent1_arg1_end+1]\n",
    "            sent1_arg2_w = sent1_lst[sent1_arg2_start:sent1_arg2_end+1]\n",
    "            sent2_arg1_w = sent2_lst[sent2_arg1_start:sent2_arg1_end+1]\n",
    "            sent2_arg2_w = sent2_lst[sent2_arg2_start:sent2_arg2_end+1]\n",
    "            \n",
    "            d = {\"first\": sent1_with_args, \"second\": sent2, \"second_with_arguments\": sent2_with_args, \"query_first\": query1, \"query_second\": query2, \n",
    "                 \"first_arg1\": (sent1_arg1_start, sent1_arg1_end), \"first_arg2\": (sent1_arg2_start, sent1_arg2_end), \n",
    "                 \"second_arg1\": (sent2_arg1_start, sent2_arg1_end),\n",
    "                 \"second_arg2\": (sent2_arg2_start, sent2_arg2_end), \"first_arg1_words\": sent1_arg2_w,\n",
    "                \"first_arg2_words\": sent1_arg2_w, \"second_arg1_words\": sent2_arg1_w, \"second_arg2_words\": sent2_arg2_w,\n",
    "                \"query_id\": id}\n",
    "            relation_examples.append(d)\n",
    "    \n",
    "    random.shuffle(relation_examples)\n",
    "    relation_examples = relation_examples[:NUM_PAIRS_PER_RELATION]\n",
    "    data.extend(relation_examples)\n",
    "    \n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 23 22 12 36 33 54 11 22 13 11]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(d[\"first\"].split(\" \")) + len(d[\"second\"].split(\" \")) for d in data]\n",
    "data = [d for i,d in enumerate(data) if lengths[i] < 250]\n",
    "data = [d for d in data if (d[\"second_arg1\"] != d[\"second_arg2\"]) and (d[\"first_arg1\"] != d[\"first_arg2\"])]\n",
    "data = [d for d in data if d[\"first_arg1_words\"] != [''] and d[\"first_arg2_words\"] != ['']\n",
    "       and d[\"second_arg1_words\"] != [''] and d[\"second_arg2_words\"] != ['']]\n",
    "ids_subset = np.random.choice(list(ids), size = int(0.2 * len(ids)))\n",
    "data_train = [d for d in data if d[\"query_id\"] not in ids_subset]\n",
    "data_dev = [d for d in data if d[\"query_id\"] in ids_subset]\n",
    "print(ids_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "    \n",
    "with open(\"data_dev.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data_dev, f)\n",
    "    \n",
    "with open(\"data_train.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data_train, f)\n",
    "    \n",
    "import json\n",
    "\n",
    "def write(data, fname):\n",
    "    \n",
    "    with open(fname, \"w\") as f:\n",
    "        for d in data:\n",
    "            first, second = d[\"first\"], d[\"second_with_arguments\"]\n",
    "            first_arg1 = d[\"first_arg1\"]\n",
    "            first_arg2 = d[\"first_arg2\"]\n",
    "            second_arg1 = d[\"second_arg1\"]\n",
    "            second_arg2 = d[\"second_arg2\"]\n",
    "        \n",
    "            elems = [first, second, first_arg1, first_arg2, second_arg1, second_arg2]\n",
    "            keys = [\"first\", \"second\", \"first_arg1\", \"first_arg2\"]\n",
    "        \n",
    "            f.write(json.dumps(d) + \"\\n\")\n",
    "        \n",
    "            #f.write(d[\"first\"] + \"\\t\" + d[\"second\"] + \"\\t\" + d[\"query\"] + \"\\t\" + \"-\".join(d[\"first_arg1\"])+ \"\\n\")\n",
    "            \n",
    "    \n",
    "write(data, \"data.txt\")\n",
    "write(data_dev, \"data_dev.txt\")\n",
    "write(data_train, \"data_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19738, 4708)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), len(data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure of coronavirus 3C-like proteinase contains three domains , the first two domains form a chymotrypsin fold , which is responsible for the catalytic reaction , and <<ARG1:the extra helical domain of the enzyme>> plays an important role in controlling the associationdissociation equilibrium and regulating <<ARG2:the activity and specificity of the enzyme>> [ 6 ] . (28, 34) (46, 52)\n",
      "In rotavirus infected cells an increased Akt phosphorylation depends on a direct interaction between <<ARG1:the viral nonstructural protein NSP1>> and <<ARG2:PI3 K>> .\n",
      "---------------\n",
      "the $enzyme <>arg1:A $[l]regulates the $enzyme <>arg2:B\n",
      "the $[l]interaction between <>arg1:[e=PROTEIN]A and <>arg2:[e=PROTEIN]B\n",
      "============================\n",
      "Specimens 27 and 199 reacted strongly with all Ad41 DNA probes , although types Ad2 and <<ARG1:Ad5>> emerged from <<ARG2:culture>> . (16, 16) (19, 19)\n",
      "Kohl et al. [ 25 ] carried out genome pyrosequencing of bat adenovirus 2 and found there was an interspecies transmission during evolution of <<ARG1:Canine adenovirus>> 1 and 2 from <<ARG2:Vespertilionid bat adenovirus>> .\n",
      "---------------\n",
      "<>arg1:[e=ORGANISM|ORGANISM_SUBSTANCE]flu $[l=evolve|emerge|diverge]evolved $from <>arg2:something\n",
      "the $evolution $of <>arg1:[e=ORGANISM|ORGANISM_SUBSTANCE]flu $from <>arg2:[e=ORGANISM|ORGANISM_SUBSTANCE]something\n",
      "============================\n",
      "Also , <<ARG1:a high blood eosinophil count>> may be a sign of an increased risk of persistent wheezing and atopy [ <<ARG2:35>> ] . (2, 6) (21, 21)\n",
      "16 , 28 <<ARG1:These lower blood counts>> might be an important warning sign in <<ARG2:the early identification of COVID-19>> in fever clinics .\n",
      "---------------\n",
      "$high $blood <>arg1:glocuse is a $sign of <>arg2:something\n",
      "low $blood <>arg1:glocuse is a $[l=sign|indication|symptom]sign of <>arg2:something\n",
      "============================\n",
      "The SARS-CoV-2 virus , as <<ARG1:it>> has been named by <<ARG2:the WHO>> , seems to be closely related to bat and pangolin coronaviruses , but a final confirmation has not yet been given [ 4 ] . (5, 5) (10, 11)\n",
      "around <<ARG1:a central value k>> 0 is described by <<ARG2:the integral>>\n",
      "---------------\n",
      "<>arg1:something was $named $by <>arg2:someone\n",
      "<>arg1:something was first $[w=described|reported|observed]described $by <>arg2:someone\n",
      "============================\n",
      "<<ARG2:It>> can be a symptom of both pulmonary conditions , such as asthma ( Dicpinigaitis 2006b ) , chronic obstructive pulmonary disease ( COPD ) ( Braman 2006 ) , idiopathic pulmonary fibrosis ( Hope-Gill et al. 2003 ) , <<ARG1:upper-airway cough syndrome>> ( postnasal drip ) ( Pratter 2006 ) and lung cancer ( Kvale 2006 ) , and extrapulmonary conditions , such as gastro-oesophageal reflux ( Irwin 2006 ) . (40, 42) (0, 0)\n",
      "Objective : <<ARG2:Bloody nipple discharge>> ( BND ) is an important clinical symptom in mammary disorders , especially <<ARG1:cancers>> .\n",
      "---------------\n",
      "<>arg2:something is $a $[w=symptom|sign]sign $of <>arg1:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma\n",
      "<>arg2:something is a $symptom of <>arg1:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma\n",
      "============================\n",
      "<<ARG1:Liver transplant>> ( LT ) is an effective treatment for <<ARG2:hepatitis B virus (HBV)-related end-stage liver disease>> ( such as acute or chronic liver failure , cirrhosis , hepatocellular carcinoma and so on ) . (0, 1) (10, 16)\n",
      "<<ARG1:Acute cardiotoxicity>> occurred in 5 patients from 1 to 15 years after transplant , and required <<ARG2:heart transplant>> in 2 patients .\n",
      "---------------\n",
      "<>arg1:[e=ORGAN]Liver $transplant is a $[l=treatment|measure|operation]treatment for <>arg2:[e=DISEASE]flu\n",
      "<>arg1:[e=DISEASE]disease may $[l=require|necessitate|need]require <>arg2:[e=ORGAN]lung $transplant\n",
      "============================\n",
      "Expression of the angiotensin-converting enzyme 2 ( ACE2 ) , identified as a crucial factor that facilitates <<ARG1:SARS-CoV-2 virus>> to bind and enter <<ARG2:host cells>> , is substantially increased in patients with diabetes and hypertension , who are often treated with ACE inhibitors and angiotensin II type-I receptor blockers [ 32 ] . (17, 18) (23, 24)\n",
      "For example , <<ARG1:HCV>> can interfere with TPO production by damaging <<ARG2:the liver tissue>> ( 136 ) .\n",
      "---------------\n",
      "<>arg1:influenza $virus $[l]enters the <>arg2:lung $cells\n",
      "<>arg1:[e=COVID-19|ORGANISM|ORGANISM_SUBDIVISION|ORGANISM_SUBSTANCE]COVID-19 $[l=invade|enter|infect|damage]invades <>arg2:lung $tissue\n",
      "============================\n",
      "Until now , it is unclear how long the virus could stay on surfaces , but preliminary studies suggest that coronaviruses including ( <<ARG1:SARS-CoV-2>> ) may remain infectious on <<ARG2:surfaces>> for a few hours to several days depending on the different conditions including type of surface , humidity , and temperature . (23, 23) (29, 29)\n",
      "Additionally , it has also been found that <<ARG1:SARS-CoV-2>> can last up to 72 hours on plastic , cardboard , and <<ARG2:J>> o u r n a l P r e -p r o o f stainless steel suggesting that donning and doffing of used , but non-soiled , respirators requires great caution so as not to contaminate the inside of the mask or oneself 50 .\n",
      "---------------\n",
      "<>arg1:[e]COVID-19 $[l]remains $infectious $on <>arg2:surfaces\n",
      "<>arg1:[e]COVID-19 $[l]can $last $on <>arg2:surfaces\n",
      "============================\n",
      "Coudron , Rodríguez-Martínez et al. , and Pires et al. [ 5 , 6 , 8 ] describe that <<ARG2:the AmpC enzyme>> is inhibited by <<ARG1:boronic acid>> and cloxacillin . (25, 26) (19, 21)\n",
      "In addition , two transporters , organic anion-transporting polypeptide 1B1 ( OATP1B1 ) and <<ARG2:breast cancer resistance protein>> ( BCRP ) , also known as ATP-binding cassette ( ABC ) transporter G2 , are also inhibited by <<ARG1:PIs>> affecting transporter-mediated interaction of statins [ 29 , 30 ] .\n",
      "---------------\n",
      "<>arg2:something $enzyme is $inhibited by <>arg1:something\n",
      "<>arg2:something $protein is $inhibited by <>arg1:something\n",
      "============================\n",
      "Screenshots of Au-ASOmix nanoparticles as observed through zetaview after the addition of total RNA ( 1 ng/µL ) isolated from <<ARG2:the Vero cells>> infected with <<ARG1:SARS-CoV-2>> . (25, 25) (20, 22)\n",
      "Finally , we addressed whether <<ARG1:SARS-CoV-2>> infects <<ARG2:human heart tissue>> by using living human cardiac tissue slices , which were obtained from explanted hearts 16 ( Figure 4a-f ) .\n",
      "---------------\n",
      "<>arg2:lung $[l=cell|tissue]cells $infected by <>arg1:[e]COVID-19\n",
      "<>arg1:[e=COVID-19|ORGANISM|ORGANISM_SUBDIVISION|ORGANISM_SUBSTANCE]COVID-19 $[l=invade|enter|infect|damage]invades <>arg2:lung $tissue\n",
      "============================\n",
      "Use of corticosteroids ( <<ARG1:CCS>> ) is not definitely recommended during <<ARG2:viral pulmonary infections>> . (4, 4) (11, 13)\n",
      "Similar to other studies , <<ARG1:ribavirin>> and corticosteroids were not effective in treating <<ARG2:SARS patients>> in Liu et al 's report .\n",
      "---------------\n",
      "<>arg1:[e=CHEMICAL|SIMPLE_CHEMICAL]paracetamol $is $not $recommended against <>arg2:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma\n",
      "<>arg1:[e=CHEMICAL|SIMPLE_CHEMICAL]paracetamol is $not $effective in treating <>arg2:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma\n",
      "============================\n",
      "The presence of the viral entry machinery in the kidney suggests that <<ARG1:the SARS-CoV-2 virus>> could invade and injure the kidney cells , like <<ARG2:other known host cells>> such as epithelial cells in the upper airway and type II pneumocytes of the lung . (12, 14) (24, 27)\n",
      "The most widely studied nodavirus is <<ARG1:Flock house virus>> ( FHV ) , which can infect a wide range of <<ARG2:host cells>> , including mammalian , insect , plant , and yeast cells ( Johnson and Ball , 1997 ; Lu et al. , 2005 ; Price et al. , 1996 ; Selling et al. , 1990 ; Venter and Schneemann , 2008 ) .\n",
      "---------------\n",
      "<>arg1:influenza $virus $[l]invades to <>arg2:lung $cells\n",
      "<>arg1:influenza $virus $[l]infects <>arg2:lung $cells\n",
      "============================\n",
      "One limitation of this study is that the scope of research was confined to the outbreak of <<ARG2:MERS-CoV>> in <<ARG1:one country>> . (19, 20) (17, 17)\n",
      "An outbreak of <<ARG2:SARS-CoV>> occurred in <<ARG1:East>> and Southeast Asia in early spring of 2003 .\n",
      "---------------\n",
      "$outbreak $of <>arg2:[e]MERS-CoV $in <>arg1:[e]France\n",
      "$outbreak $of <>arg2:[e]MERS-CoV $occurred $in <>arg1:[e]France\n",
      "============================\n",
      "Instead , it binds to <<ARG1:the surface>> responsible for <<ARG2:caspase homodimerization>> , thus preventing the process . (5, 6) (9, 10)\n",
      "Various feline enteric coronavirus strains produce clinical signs that resemble <<ARG2:those>> produced by TGEV in swine and by <<ARG1:CCV>> in puppies .\n",
      "---------------\n",
      "<>arg1:[e=CELLULAR_COMPONENT]mitochondria is $responsible for <>arg2:energy production\n",
      "<>arg2:energy $[l=produce|create|generate]produced by the <>arg1:[e=CELLULAR_COMPONENT]mitochondria\n",
      "============================\n",
      "<<ARG1:The SARS virus>> can remain viable on <<ARG2:environmental surfaces>> for a few hours . (0, 2) (7, 8)\n",
      "For the surface survival of virus , <<ARG1:SARS-CoV-2>> can remain viable and infectious on <<ARG2:surfaces>> up to days , so common surface transmission of the virus is highly possible [ 21 ] .\n",
      "---------------\n",
      "<>arg1:flu $virus $[l]remains $viable $on <>arg2:surfaces\n",
      "<>arg1:[e]COVID-19 $[l]remains $infectious $on <>arg2:surfaces\n",
      "============================\n",
      "Again <<ARG1:an influenza virus>> transmitted from <<ARG2:aquatic birds>> into the chicken population and for about a year the Mexicans did not know that a new virus was there . (1, 3) (6, 7)\n",
      "Rather , <<ARG2:bats>> are the natural reservoir of <<ARG1:a wide variety of CoVs>> , including SARS-CoV-like and MERS-CoV-like viruses [ 11 ] [ 12 ] [ 13 ] .\n",
      "---------------\n",
      "<>arg1:flu $virus $transmitted $from <>arg2:[e]bats\n",
      "<>arg2:[e]bats are a $reservoir $of <>arg1:[e]something\n",
      "============================\n",
      "Indeed , <<ARG1:PPE>> is manufactured from various polymers ( polystyrene , polypropylene , polyethylene , polyvinyl chloride , polyethylene terephthalate , etc . ) and <<ARG2:metallic compounds>> [ 9 ] [ 10 ] [ 11 ] . (2, 2) (25, 26)\n",
      "NS5B can also catalyze both de novo synthesis from <<ARG1:a single-stranded template>> and primer extension from the subsequent RNA duplex or from a pre-annealed template/primer duplex [ <<ARG2:16>> ] .\n",
      "---------------\n",
      "<>arg1:something is $manufactured $from <>arg2:something\n",
      "$[w=synthesis|generation]synthesis of <>arg1:something $from <>arg2:something\n",
      "============================\n",
      "<<ARG2:BACKGROUND Obesity>> is a leading comorbidity in psoriatic disease , including both psoriasis ( <<ARG1:PsO>> ) and psoriatic arthritis ( PsA ) , and is associated with adverse metabolic and cardiovascular ( CV ) outcomes . (14, 14) (0, 1)\n",
      "<<ARG1:Rats>> are susceptible to <<ARG2:MCMV>> after intraperitoneal inoculation ( Smith et ul . , 1986 ) .\n",
      "---------------\n",
      "<>arg2:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma :is a common $comorbidity in people with <>arg1:[e=DISEASE|PATHOLOGICAL_FORMATION]asthma\n",
      "<>arg1:something $[l]are $susceptible $to <>arg2:something\n",
      "============================\n",
      "Skin breakdown occurs either due to the sclerosant infiltrating <<ARG2:skin>> that is already affected by <<ARG1:the malformation>> and therefore thinned and fragile . (15, 16) (9, 9)\n",
      "Serious cases , particularly those presenting with <<ARG1:glomerulonephritis>> or other forms of <<ARG2:serious organ involvement>> , may require treatment with high doses of glucocorticoids and cytotoxic agents .\n",
      "---------------\n",
      "<>arg2:[e=ORGAN]liver, which may be $[l=affect|influence|damage]affected from <>arg1:[e=DISEASE]flu\n",
      "<>arg1:[e=DISEASE]flu with <>arg2:[e=ORGAN]liver $involvement\n",
      "============================\n",
      "Even in the absence of detectable viral growth within the CNS after day 7 ( data not shown ) , approximately 40 % of mice infected with <<ARG1:10 PFU of S4R22>> still died within 3 weeks ( <<ARG2:Fig. 1A>> ) . (27, 30) (37, 38)\n",
      "Whereas <<ARG1:CP-BVDV>> causes vacuolation and death of certain cell lines within <<ARG2:days of inoculation>> into cell culture , NCP-BVDV inoculation into cell culture results in inapparent infection .\n",
      "---------------\n",
      "animals $infected with <>arg1:something $died $within <>arg2:something\n",
      "<>arg1:something $causes $death $within <>arg2:something\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(data[i][\"first\"], data[i][\"first_arg1\"], data[i][\"first_arg2\"])\n",
    "    print(data[i][\"second_with_arguments\"])\n",
    "    print(\"---------------\")\n",
    "    print(data[i][\"query_first\"])\n",
    "    print(data[i][\"query_second\"])\n",
    "    print(\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24437\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(d[\"first\"].split(\" \")) + len(d[\"second\"].split(\" \")) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67.92299198429576, 22.057857454680867, 237, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths), np.std(lengths), np.max(lengths), np.min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([l for l in lengths if l > 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
