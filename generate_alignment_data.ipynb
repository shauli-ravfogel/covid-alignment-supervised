{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import spike_queries\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel, AutoTokenizer, AutoModel, PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries.txt\", \"r\") as f:\n",
    "    queries = f.readlines()\n",
    "    queries = [l.strip() for l in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [06:09<00:00,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"covid19\"\n",
    "num_results = 100\n",
    "query_type = \"syntactic\"\n",
    "\n",
    "for q in tqdm.tqdm(queries, total = len(queries)):\n",
    "    df = spike_queries.perform_query(q, dataset_name, num_results, query_type) #previously: word=Hawaii\n",
    "    queries2results[q] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries2results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(queries2results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries2results.pickle\", \"rb\") as f:\n",
    "    queries2results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1225\n",
      "10000\n",
      "10000\n",
      "36\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "81\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "2601\n",
      "10000\n",
      "5041\n",
      "10000\n",
      "10000\n",
      "576\n",
      "10000\n",
      "4096\n",
      "10000\n",
      "10000\n",
      "169\n",
      "6241\n",
      "10000\n",
      "4\n",
      "10000\n",
      "841\n",
      "3721\n",
      "10000\n",
      "10000\n",
      "441\n",
      "2704\n",
      "169\n",
      "10000\n",
      "3969\n",
      "361\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "576\n",
      "10000\n",
      "16\n",
      "25\n",
      "196\n",
      "10000\n",
      "10000\n",
      "576\n",
      "169\n",
      "3600\n",
      "529\n",
      "3844\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "576\n",
      "576\n",
      "10000\n",
      "121\n",
      "25\n",
      "529\n",
      "289\n",
      "10000\n",
      "2916\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for q in queries:\n",
    "    #print(q)\n",
    "    results = queries2results[q]\n",
    "    sents, arg1_first, arg2_first = results[\"sentence_text\"].tolist(), results[\"arg1_first_index\"].tolist(), results[\"arg2_first_index\"].tolist()\n",
    "    arg1_last, arg2_last = results[\"arg1_last_index\"].tolist(), results[\"arg2_last_index\"].tolist()\n",
    "    \n",
    "    sents_with_args = []\n",
    "    for s,arg1_ind,arg2_ind, arg1_ind_last, arg2_ind_last in zip(sents,arg1_first,arg2_first, arg1_last, arg2_last):\n",
    "        s_lst = s.split(\" \")\n",
    "        if arg1_ind > arg2_ind:\n",
    "            arg1_ind, arg2_ind = arg2_ind, arg1_ind\n",
    "            arg1_ind_last, arg2_ind_last = arg2_ind_last, arg1_ind_last\n",
    "            arg1_str, arg2_str = \"ARG2:\", \"ARG1:\"\n",
    "        else:\n",
    "            arg1_str, arg2_str = \"ARG1:\", \"ARG2:\"\n",
    "        s_with_args = s_lst[:arg1_ind] + [arg1_str+s_lst[arg1_ind]] + s_lst[arg1_ind+1:arg2_ind] + [arg2_str+s_lst[arg2_ind]] + s_lst[arg2_ind+1:]\n",
    "        s_with_args = \" \".join(s_with_args)\n",
    "        sents_with_args.append({\"sent\": s_with_args, \"start_1\": arg1_ind, \"start_2\": arg2_ind, \"end_1\": arg1_ind_last,\n",
    "                               \"end_2\": arg2_ind_last})\n",
    "    \n",
    "    max_number_of_pairs = 100\n",
    "    pairs = list(itertools.product(sents_with_args, repeat=2))\n",
    "    print(len(pairs))\n",
    "    random.shuffle(pairs)\n",
    "    for pair in pairs[:max_number_of_pairs]:\n",
    "        data.append({\"first\": pair[0][\"sent\"], \"second\": pair[1][\"sent\"], \"query\": q, \"first_arg1\": (pair[0][\"start_1\"], pair[0][\"end_1\"]),\n",
    "                    \"first_arg2\": (pair[0][\"start_2\"], pair[0][\"end_2\"]), \"second_arg1\": (pair[1][\"start_1\"], pair[1][\"end_1\"]),\n",
    "                   \"second_arg2\": (pair[1][\"start_2\"], pair[1][\"end_2\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "data[0]\n",
    "\n",
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    for d in data:\n",
    "        first, second = d[\"first\"], d[\"second\"]\n",
    "        first_arg1 = d[\"first_arg1\"]\n",
    "        first_arg2 = d[\"first_arg2\"]\n",
    "        second_arg1 = d[\"second_arg1\"]\n",
    "        second_arg2 = d[\"second_arg2\"]\n",
    "        \n",
    "        elems = [first, second, first_arg1, first_arg2, second_arg1, second_arg2]\n",
    "        keys = [\"first\", \"second\", \"first_arg1\", \"first_arg2\"]\n",
    "        \n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "        #f.write(d[\"first\"] + \"\\t\" + d[\"second\"] + \"\\t\" + d[\"query\"] + \"\\t\" + \"-\".join(d[\"first_arg1\"])+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': '3e6 According to the report of the WHO and China Joint Mission on Coronavirus Disease 2019 ( COVID- 19 ) , it is a zoonotic virus , based on the data available , ARG1:bats seems to be the reservoir of COVID-19 ARG2:virus .',\n",
       " 'second': 'Furthermore , sequence analyses indicated the existence of a much greater genetic diversity of SARS-like-CoVs in bats than of SARS-CoVs in civets or humans , which supports the notion that SARS-CoV is a member of this novel coronavirus group and that ARG1:bats are a natural reservoir for ARG2:it .',\n",
       " 'query': 'arg1:[e]bats are a $reservoir of arg2:[e]something',\n",
       " 'first_arg1': (33, 33),\n",
       " 'first_arg2': (41, 41),\n",
       " 'second_arg1': (41, 41),\n",
       " 'second_arg2': (47, 47)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(data)\n",
    "data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>arg1</th>\n",
       "      <th>arg2</th>\n",
       "      <th>title</th>\n",
       "      <th>article_link</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>pdf_sha</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>msft_id</th>\n",
       "      <th>arg1_first_index</th>\n",
       "      <th>arg1_last_index</th>\n",
       "      <th>arg2_first_index</th>\n",
       "      <th>arg2_last_index</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>paragraph_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8903</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>depletion</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7173247</td>\n",
       "      <td>10.1016/0168-1702(95)00092-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8837897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8903</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>macrophages</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7173247</td>\n",
       "      <td>10.1016/0168-1702(95)00092-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8837897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8903</td>\n",
       "      <td>Mouse</td>\n",
       "      <td>alterations</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7173247</td>\n",
       "      <td>10.1016/0168-1702(95)00092-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8837897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8903</td>\n",
       "      <td>hepatitis</td>\n",
       "      <td>depletion</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7173247</td>\n",
       "      <td>10.1016/0168-1702(95)00092-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8837897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8903</td>\n",
       "      <td>hepatitis</td>\n",
       "      <td>macrophages</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7173247</td>\n",
       "      <td>10.1016/0168-1702(95)00092-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8837897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "      <td>Mouse hepatitis virus infection of mice causes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>540072</td>\n",
       "      <td>Rhinovirus</td>\n",
       "      <td>cells</td>\n",
       "      <td>Rhinoviruses</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7150364</td>\n",
       "      <td>10.1016/b978-012373960-5.00610-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Rhinovirus infection causes lysis of the infec...</td>\n",
       "      <td>Rhinovirus has a propensity to affect young ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>540072</td>\n",
       "      <td>Rhinovirus</td>\n",
       "      <td>decrease</td>\n",
       "      <td>Rhinoviruses</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7150364</td>\n",
       "      <td>10.1016/b978-012373960-5.00610-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Rhinovirus infection causes lysis of the infec...</td>\n",
       "      <td>Rhinovirus has a propensity to affect young ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>546506</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>cytokine</td>\n",
       "      <td>Comparison of confirmed COVID‐19 with SARS and...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7300470</td>\n",
       "      <td>10.1002/rmv.2112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32502331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>128 Another study reported that following COVI...</td>\n",
       "      <td>Most of the patients with coronavirus had abno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>546506</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>storm</td>\n",
       "      <td>Comparison of confirmed COVID‐19 with SARS and...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7300470</td>\n",
       "      <td>10.1002/rmv.2112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32502331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>128 Another study reported that following COVI...</td>\n",
       "      <td>Most of the patients with coronavirus had abno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>612087</td>\n",
       "      <td>virus</td>\n",
       "      <td>activation</td>\n",
       "      <td>Are virus infections triggers for autoimmune d...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...</td>\n",
       "      <td>PMC7135185</td>\n",
       "      <td>10.1016/s0196-4399(02)80019-8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32287669.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>A related mechanism is bystander activation , ...</td>\n",
       "      <td>Viruses have long been thought to trigger auto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id        arg1         arg2  \\\n",
       "0          8903       Mouse    depletion   \n",
       "1          8903       Mouse  macrophages   \n",
       "2          8903       Mouse  alterations   \n",
       "3          8903   hepatitis    depletion   \n",
       "4          8903   hepatitis  macrophages   \n",
       "..          ...         ...          ...   \n",
       "95       540072  Rhinovirus        cells   \n",
       "96       540072  Rhinovirus     decrease   \n",
       "97       546506    COVID-19     cytokine   \n",
       "98       546506    COVID-19        storm   \n",
       "99       612087       virus   activation   \n",
       "\n",
       "                                                title  \\\n",
       "0   Mouse hepatitis virus infection of mice causes...   \n",
       "1   Mouse hepatitis virus infection of mice causes...   \n",
       "2   Mouse hepatitis virus infection of mice causes...   \n",
       "3   Mouse hepatitis virus infection of mice causes...   \n",
       "4   Mouse hepatitis virus infection of mice causes...   \n",
       "..                                                ...   \n",
       "95                                       Rhinoviruses   \n",
       "96                                       Rhinoviruses   \n",
       "97  Comparison of confirmed COVID‐19 with SARS and...   \n",
       "98  Comparison of confirmed COVID‐19 with SARS and...   \n",
       "99  Are virus infections triggers for autoimmune d...   \n",
       "\n",
       "                                         article_link      pmc_id  \\\n",
       "0   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7173247   \n",
       "1   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7173247   \n",
       "2   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7173247   \n",
       "3   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7173247   \n",
       "4   https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7173247   \n",
       "..                                                ...         ...   \n",
       "95  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7150364   \n",
       "96  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7150364   \n",
       "97  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7300470   \n",
       "98  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7300470   \n",
       "99  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7...  PMC7135185   \n",
       "\n",
       "                                 doi  pdf_sha   pubmed_id  msft_id  \\\n",
       "0       10.1016/0168-1702(95)00092-5      NaN   8837897.0      NaN   \n",
       "1       10.1016/0168-1702(95)00092-5      NaN   8837897.0      NaN   \n",
       "2       10.1016/0168-1702(95)00092-5      NaN   8837897.0      NaN   \n",
       "3       10.1016/0168-1702(95)00092-5      NaN   8837897.0      NaN   \n",
       "4       10.1016/0168-1702(95)00092-5      NaN   8837897.0      NaN   \n",
       "..                               ...      ...         ...      ...   \n",
       "95  10.1016/b978-012373960-5.00610-9      NaN         NaN      NaN   \n",
       "96  10.1016/b978-012373960-5.00610-9      NaN         NaN      NaN   \n",
       "97                  10.1002/rmv.2112      NaN  32502331.0      NaN   \n",
       "98                  10.1002/rmv.2112      NaN  32502331.0      NaN   \n",
       "99     10.1016/s0196-4399(02)80019-8      NaN  32287669.0      NaN   \n",
       "\n",
       "    arg1_first_index  arg1_last_index  arg2_first_index  arg2_last_index  \\\n",
       "0                  0                0                 8                8   \n",
       "1                  0                0                13               13   \n",
       "2                  0                0                17               17   \n",
       "3                  1                1                 8                8   \n",
       "4                  1                1                13               13   \n",
       "..               ...              ...               ...              ...   \n",
       "95                 0                0                 7                7   \n",
       "96                 0                0                10               10   \n",
       "97                 6                6                18               18   \n",
       "98                 6                6                19               19   \n",
       "99                 8                8                 5                5   \n",
       "\n",
       "                                        sentence_text  \\\n",
       "0   Mouse hepatitis virus infection of mice causes...   \n",
       "1   Mouse hepatitis virus infection of mice causes...   \n",
       "2   Mouse hepatitis virus infection of mice causes...   \n",
       "3   Mouse hepatitis virus infection of mice causes...   \n",
       "4   Mouse hepatitis virus infection of mice causes...   \n",
       "..                                                ...   \n",
       "95  Rhinovirus infection causes lysis of the infec...   \n",
       "96  Rhinovirus infection causes lysis of the infec...   \n",
       "97  128 Another study reported that following COVI...   \n",
       "98  128 Another study reported that following COVI...   \n",
       "99  A related mechanism is bystander activation , ...   \n",
       "\n",
       "                                       paragraph_text  \n",
       "0   Mouse hepatitis virus infection of mice causes...  \n",
       "1   Mouse hepatitis virus infection of mice causes...  \n",
       "2   Mouse hepatitis virus infection of mice causes...  \n",
       "3   Mouse hepatitis virus infection of mice causes...  \n",
       "4   Mouse hepatitis virus infection of mice causes...  \n",
       "..                                                ...  \n",
       "95  Rhinovirus has a propensity to affect young ch...  \n",
       "96  Rhinovirus has a propensity to affect young ch...  \n",
       "97  Most of the patients with coronavirus had abno...  \n",
       "98  Most of the patients with coronavirus had abno...  \n",
       "99  Viruses have long been thought to trigger auto...  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries2results[queries[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, train_dataset: Dataset, dev_dataset: Dataset, batch_size, device: str, mode: str = \"eval\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=config)    \n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        \n",
    "        if mode == \"eval\":\n",
    "            \n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            \n",
    "        self.train_gen = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "        self.dev_gen = torch.utils.data.DataLoader(self.dev_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "        self.acc = None\n",
    "\n",
    "        \n",
    "    def tokenize(self, original_sentence: List[str]) -> Tuple[List[str], Dict[int, int]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        bert_tokens: The sentence, tokenized by BERT tokenizer.\n",
    "        orig_to_tok_map: An output dictionary consisting of a mapping (alignment) between indices in the original tokenized sentence, and indices in the sentence tokenized by the BERT tokenizer. See https://github.com/google-research/bert\n",
    "        \"\"\"\n",
    "\n",
    "        bert_tokens = [\"[CLS]\"]\n",
    "        orig_to_tok_map = {}\n",
    "        tok_to_orig_map = {}\n",
    "        has_subwords = False\n",
    "        is_subword = []\n",
    "\n",
    "        for i, w in enumerate(original_sentence):\n",
    "            tokenized_w = self.tokenizer.tokenize(w)\n",
    "            has_subwords = len(tokenized_w) > 1\n",
    "            is_subword.append(has_subwords)\n",
    "            bert_tokens.extend(tokenized_w)\n",
    "\n",
    "            orig_to_tok_map[i] = len(bert_tokens) - 1\n",
    "\n",
    "        tok_to_orig_map = {}\n",
    "\n",
    "        keys = list(sorted(orig_to_tok_map.keys()))\n",
    "        for k, k2 in zip(keys, keys[1:]):\n",
    "            for k3 in range(orig_to_tok_map[k], orig_to_tok_map[k2]):\n",
    "                tok_to_orig_map[k3] = k\n",
    "\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "        \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "\n",
    "        return (bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        states = outputs[0][0] #[seq_len, 768]\n",
    "        return states\n",
    "    \n",
    "    def forward_with_loss_calculation(self, x, sent1_idx, sent2_idx, orig_to_tok_map, l):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        states = outputs[0][0] #[seq_len, 768]\n",
    "        states = states/torch.norm(states, dim = 1, keepdim = True)\n",
    "        \n",
    "        gold_arg1_sent1 = orig_to_tok_map[sent1_idx[0].detach().cpu().numpy().item()]\n",
    "        gold_arg2_sent1 = orig_to_tok_map[sent1_idx[1].detach().cpu().numpy().item()]\n",
    "        \n",
    "        sent1_arg1_vec, sent1_arg2_vec = states[gold_arg1_sent1], states[orig_to_tok_map[gold_arg2_sent1]\n",
    "        \n",
    "        #states_sent2 = states[orig_to_tok_map[l]:]  \n",
    "        \n",
    "        sims_arg1 = sent1_arg1_vec @ states.T\n",
    "        sims_arg2 = sent1_arg2_vec @ states.T\n",
    "        \n",
    "        gold_arg1_sent2, gold_arg2_sent2 = orig_to_tok_map[sent2_idx[0].detach().cpu().numpy().item()], orig_to_tok_map[sent2_idx[1].detach().cpu().numpy().item()]\n",
    "        sim_arg1_gold = sims_arg1[gold_arg1_sent2]\n",
    "        sim_arg2_gold = sims_arg2[gold_arg2_sent2]\n",
    "        \n",
    "        idx_arg1 = torch.argsort(sims_arg1)\n",
    "        idx_arg2 = torch.argsort(sims_arg2)\n",
    "        \n",
    "        most_sim_arg1, most_sim_arg2 = sims_arg1[idx_arg1[-2]], sims_arg2[idx_arg2[-2]]\n",
    "        \n",
    "        alpha = 0.1\n",
    "        loss_arg1 = torch.max(torch.zeros(1).to(self.device), -sim_arg1_gold + most_sim_arg1 + alpha)\n",
    "        loss_arg2 = torch.max(torch.zeros(1).to(self.device), -sim_arg2_gold + most_sim_arg2 + alpha)\n",
    "                                                                         \n",
    "        if idx_arg1[-2] == gold_arg1_sent2:\n",
    "            loss_arg1 = torch.zeros(1).to(self.device)\n",
    "                                                                        \n",
    "        if idx_arg2[-2] == gold_arg2_sent2:\n",
    "            loss_arg2 = torch.zeros(1).to(self.device)                                                      \n",
    "                                                                         \n",
    "        loss = loss_arg1 + loss_arg2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        sents_concat, sent1_idx, sent2_idx, l = batch  \n",
    "        bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = self.tokenize(sents_concat[0].split(\" \"))        \n",
    "        loss = self.forward_with_loss_calculation(tokens_tensor, sent1_idx, sent2_idx, orig_to_tok_map, l)\n",
    "\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "\n",
    "        sents_concat, sent1_idx, sent2_idx, l = batch\n",
    "        \n",
    "        bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = self.tokenize(sents_concat[0].split(\" \"))        \n",
    "        loss = self.forward_with_loss_calculation(tokens_tensor, sent1_idx, sent2_idx, orig_to_tok_map, l)\n",
    "\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        print(\"Loss is {}\".format(avg_loss))\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.SGD(self.parameters(), lr=0.005, momentum=0.9)\n",
    "        return torch.optim.Adam(self.parameters(), weight_decay=1e-4)\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self.train_gen\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return self.dev_gen\n",
    "    \n",
    "    \n",
    "def get_prediction(sent1, sent2, model):\n",
    "\n",
    "    l = len(sent1.split(\" \")) + 1 \n",
    "    sent2 = sent2.replace(\"ARG1:\", \"\").replace(\"ARG2:\", \"\")\n",
    "    sents_concat = sent1 + \" ***** \" + sent2\n",
    "    sent1_arg1 = [i for i,w in enumerate(sent1.split(\" \")) if \"ARG1:\" in w][0]\n",
    "    sent1_arg2 = [i for i,w in enumerate(sent1.split(\" \")) if \"ARG2:\" in w][0]\n",
    "    \n",
    "    bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = model.tokenize(sents_concat.split(\" \"))\n",
    "    outputs = model.model(tokens_tensor)\n",
    "    states = outputs[0][0] #[seq_len, 768]\n",
    "    states = states/torch.norm(states, dim = 1, keepdim = True)\n",
    "    sent1_arg1_vec, sent1_arg2_vec = states[orig_to_tok_map[sent1_arg1]], states[orig_to_tok_map[sent1_arg2]]\n",
    "    \n",
    "    sims_arg1 = sent1_arg1_vec @ states.T\n",
    "    sims_arg2 = sent1_arg2_vec @ states.T\n",
    "    \n",
    "    idx_arg1 = torch.argsort(sims_arg1)\n",
    "    idx_arg2 = torch.argsort(sims_arg2)\n",
    "        \n",
    "    most_sim_arg1, most_sim_arg2 = sims_arg1[idx_arg1[-2]], sims_arg2[idx_arg2[-2]]\n",
    "    ind_arg1, ind_arg2 = idx_arg1[-2].detach().cpu().numpy().item(), idx_arg2[-2].detach().cpu().numpy().item()\n",
    "    return sents_concat, tok_to_orig_map[ind_arg1], tok_to_orig_map[ind_arg2], bert_tokens[ind_arg1], bert_tokens[ind_arg2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple torch dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Dict], device = \"cpu\"):\n",
    "\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            d = self.data[index]\n",
    "            sent1, sent2 = d[\"first\"], d[\"second\"]\n",
    "            sent1_arg1, sent1_arg2 = d[\"first_arg1\"][0], d[\"first_arg2\"][0]\n",
    "            sent2_arg1, sent2_arg2 = d[\"second_arg1\"][0], d[\"second_arg2\"][0]\n",
    "            \n",
    "            l = len(sent1.split(\" \")) + 1 \n",
    "            sent2_arg1, sent2_arg2 = sent2_arg1 + l, sent2_arg2 + l\n",
    "\n",
    "            sent2 = sent2.replace(\"ARG1:\", \"\").replace(\"ARG2:\", \"\")\n",
    "            sents_concat = sent1 + \" ***** \" + sent2 #sents_concat.split(\" \")[l] is the first token in the 2nd sent\n",
    "            \n",
    "            return sents_concat, (sent1_arg1, sent1_arg2), (sent2_arg1, sent2_arg2), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = int(0.8 * len(data))\n",
    "train_dataset, dev_dataset = Dataset(data[:l], \"cpu\"), Dataset(data[l:], \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_concat, (sent1_arg1, sent1_arg2), (sent2_arg1, sent2_arg2), l = train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Steroids are ARG1:the first-line front treatment of ICI myocarditis , despite poor evidence to support this , followed by other ARG2:immune modulators , plasma exchange , or even CTLA-4 agonist ( abatacept ) infusions . ***** Moreover , the basal ECAR observed among Tyto sorted cells was similar to that 212 observed in immune populations in our previous publication [ 5 ] .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(train_dataset, dev_dataset, 1, \"cuda\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = model.tokenize(sents_concat.split(\" \"))\n",
    "indexed_tokens = model.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8575], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_with_loss_calculation(tokens_tensor, (sent1_arg1, sent1_arg2), (sent2_arg1, sent2_arg2), orig_to_tok_map, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-19 21:37:43.692 INFO    lightning: GPU available: True, used: True\n",
      "2020-10-19 21:37:43.693 INFO    lightning: CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/shauli/miniconda3/envs/py3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "2020-10-19 21:37:56.521 INFO    lightning: \n",
      "    | Name                                              | Type              | Params\n",
      "------------------------------------------------------------------------------------\n",
      "0   | model                                             | BertModel         | 109 M \n",
      "1   | model.embeddings                                  | BertEmbeddings    | 24 M  \n",
      "2   | model.embeddings.word_embeddings                  | Embedding         | 23 M  \n",
      "3   | model.embeddings.position_embeddings              | Embedding         | 393 K \n",
      "4   | model.embeddings.token_type_embeddings            | Embedding         | 1 K   \n",
      "5   | model.embeddings.LayerNorm                        | LayerNorm         | 1 K   \n",
      "6   | model.embeddings.dropout                          | Dropout           | 0     \n",
      "7   | model.encoder                                     | BertEncoder       | 85 M  \n",
      "8   | model.encoder.layer                               | ModuleList        | 85 M  \n",
      "9   | model.encoder.layer.0                             | BertLayer         | 7 M   \n",
      "10  | model.encoder.layer.0.attention                   | BertAttention     | 2 M   \n",
      "11  | model.encoder.layer.0.attention.self              | BertSelfAttention | 1 M   \n",
      "12  | model.encoder.layer.0.attention.self.query        | Linear            | 590 K \n",
      "13  | model.encoder.layer.0.attention.self.key          | Linear            | 590 K \n",
      "14  | model.encoder.layer.0.attention.self.value        | Linear            | 590 K \n",
      "15  | model.encoder.layer.0.attention.self.dropout      | Dropout           | 0     \n",
      "16  | model.encoder.layer.0.attention.output            | BertSelfOutput    | 592 K \n",
      "17  | model.encoder.layer.0.attention.output.dense      | Linear            | 590 K \n",
      "18  | model.encoder.layer.0.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "19  | model.encoder.layer.0.attention.output.dropout    | Dropout           | 0     \n",
      "20  | model.encoder.layer.0.intermediate                | BertIntermediate  | 2 M   \n",
      "21  | model.encoder.layer.0.intermediate.dense          | Linear            | 2 M   \n",
      "22  | model.encoder.layer.0.output                      | BertOutput        | 2 M   \n",
      "23  | model.encoder.layer.0.output.dense                | Linear            | 2 M   \n",
      "24  | model.encoder.layer.0.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "25  | model.encoder.layer.0.output.dropout              | Dropout           | 0     \n",
      "26  | model.encoder.layer.1                             | BertLayer         | 7 M   \n",
      "27  | model.encoder.layer.1.attention                   | BertAttention     | 2 M   \n",
      "28  | model.encoder.layer.1.attention.self              | BertSelfAttention | 1 M   \n",
      "29  | model.encoder.layer.1.attention.self.query        | Linear            | 590 K \n",
      "30  | model.encoder.layer.1.attention.self.key          | Linear            | 590 K \n",
      "31  | model.encoder.layer.1.attention.self.value        | Linear            | 590 K \n",
      "32  | model.encoder.layer.1.attention.self.dropout      | Dropout           | 0     \n",
      "33  | model.encoder.layer.1.attention.output            | BertSelfOutput    | 592 K \n",
      "34  | model.encoder.layer.1.attention.output.dense      | Linear            | 590 K \n",
      "35  | model.encoder.layer.1.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "36  | model.encoder.layer.1.attention.output.dropout    | Dropout           | 0     \n",
      "37  | model.encoder.layer.1.intermediate                | BertIntermediate  | 2 M   \n",
      "38  | model.encoder.layer.1.intermediate.dense          | Linear            | 2 M   \n",
      "39  | model.encoder.layer.1.output                      | BertOutput        | 2 M   \n",
      "40  | model.encoder.layer.1.output.dense                | Linear            | 2 M   \n",
      "41  | model.encoder.layer.1.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "42  | model.encoder.layer.1.output.dropout              | Dropout           | 0     \n",
      "43  | model.encoder.layer.2                             | BertLayer         | 7 M   \n",
      "44  | model.encoder.layer.2.attention                   | BertAttention     | 2 M   \n",
      "45  | model.encoder.layer.2.attention.self              | BertSelfAttention | 1 M   \n",
      "46  | model.encoder.layer.2.attention.self.query        | Linear            | 590 K \n",
      "47  | model.encoder.layer.2.attention.self.key          | Linear            | 590 K \n",
      "48  | model.encoder.layer.2.attention.self.value        | Linear            | 590 K \n",
      "49  | model.encoder.layer.2.attention.self.dropout      | Dropout           | 0     \n",
      "50  | model.encoder.layer.2.attention.output            | BertSelfOutput    | 592 K \n",
      "51  | model.encoder.layer.2.attention.output.dense      | Linear            | 590 K \n",
      "52  | model.encoder.layer.2.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "53  | model.encoder.layer.2.attention.output.dropout    | Dropout           | 0     \n",
      "54  | model.encoder.layer.2.intermediate                | BertIntermediate  | 2 M   \n",
      "55  | model.encoder.layer.2.intermediate.dense          | Linear            | 2 M   \n",
      "56  | model.encoder.layer.2.output                      | BertOutput        | 2 M   \n",
      "57  | model.encoder.layer.2.output.dense                | Linear            | 2 M   \n",
      "58  | model.encoder.layer.2.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "59  | model.encoder.layer.2.output.dropout              | Dropout           | 0     \n",
      "60  | model.encoder.layer.3                             | BertLayer         | 7 M   \n",
      "61  | model.encoder.layer.3.attention                   | BertAttention     | 2 M   \n",
      "62  | model.encoder.layer.3.attention.self              | BertSelfAttention | 1 M   \n",
      "63  | model.encoder.layer.3.attention.self.query        | Linear            | 590 K \n",
      "64  | model.encoder.layer.3.attention.self.key          | Linear            | 590 K \n",
      "65  | model.encoder.layer.3.attention.self.value        | Linear            | 590 K \n",
      "66  | model.encoder.layer.3.attention.self.dropout      | Dropout           | 0     \n",
      "67  | model.encoder.layer.3.attention.output            | BertSelfOutput    | 592 K \n",
      "68  | model.encoder.layer.3.attention.output.dense      | Linear            | 590 K \n",
      "69  | model.encoder.layer.3.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "70  | model.encoder.layer.3.attention.output.dropout    | Dropout           | 0     \n",
      "71  | model.encoder.layer.3.intermediate                | BertIntermediate  | 2 M   \n",
      "72  | model.encoder.layer.3.intermediate.dense          | Linear            | 2 M   \n",
      "73  | model.encoder.layer.3.output                      | BertOutput        | 2 M   \n",
      "74  | model.encoder.layer.3.output.dense                | Linear            | 2 M   \n",
      "75  | model.encoder.layer.3.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "76  | model.encoder.layer.3.output.dropout              | Dropout           | 0     \n",
      "77  | model.encoder.layer.4                             | BertLayer         | 7 M   \n",
      "78  | model.encoder.layer.4.attention                   | BertAttention     | 2 M   \n",
      "79  | model.encoder.layer.4.attention.self              | BertSelfAttention | 1 M   \n",
      "80  | model.encoder.layer.4.attention.self.query        | Linear            | 590 K \n",
      "81  | model.encoder.layer.4.attention.self.key          | Linear            | 590 K \n",
      "82  | model.encoder.layer.4.attention.self.value        | Linear            | 590 K \n",
      "83  | model.encoder.layer.4.attention.self.dropout      | Dropout           | 0     \n",
      "84  | model.encoder.layer.4.attention.output            | BertSelfOutput    | 592 K \n",
      "85  | model.encoder.layer.4.attention.output.dense      | Linear            | 590 K \n",
      "86  | model.encoder.layer.4.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "87  | model.encoder.layer.4.attention.output.dropout    | Dropout           | 0     \n",
      "88  | model.encoder.layer.4.intermediate                | BertIntermediate  | 2 M   \n",
      "89  | model.encoder.layer.4.intermediate.dense          | Linear            | 2 M   \n",
      "90  | model.encoder.layer.4.output                      | BertOutput        | 2 M   \n",
      "91  | model.encoder.layer.4.output.dense                | Linear            | 2 M   \n",
      "92  | model.encoder.layer.4.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "93  | model.encoder.layer.4.output.dropout              | Dropout           | 0     \n",
      "94  | model.encoder.layer.5                             | BertLayer         | 7 M   \n",
      "95  | model.encoder.layer.5.attention                   | BertAttention     | 2 M   \n",
      "96  | model.encoder.layer.5.attention.self              | BertSelfAttention | 1 M   \n",
      "97  | model.encoder.layer.5.attention.self.query        | Linear            | 590 K \n",
      "98  | model.encoder.layer.5.attention.self.key          | Linear            | 590 K \n",
      "99  | model.encoder.layer.5.attention.self.value        | Linear            | 590 K \n",
      "100 | model.encoder.layer.5.attention.self.dropout      | Dropout           | 0     \n",
      "101 | model.encoder.layer.5.attention.output            | BertSelfOutput    | 592 K \n",
      "102 | model.encoder.layer.5.attention.output.dense      | Linear            | 590 K \n",
      "103 | model.encoder.layer.5.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "104 | model.encoder.layer.5.attention.output.dropout    | Dropout           | 0     \n",
      "105 | model.encoder.layer.5.intermediate                | BertIntermediate  | 2 M   \n",
      "106 | model.encoder.layer.5.intermediate.dense          | Linear            | 2 M   \n",
      "107 | model.encoder.layer.5.output                      | BertOutput        | 2 M   \n",
      "108 | model.encoder.layer.5.output.dense                | Linear            | 2 M   \n",
      "109 | model.encoder.layer.5.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "110 | model.encoder.layer.5.output.dropout              | Dropout           | 0     \n",
      "111 | model.encoder.layer.6                             | BertLayer         | 7 M   \n",
      "112 | model.encoder.layer.6.attention                   | BertAttention     | 2 M   \n",
      "113 | model.encoder.layer.6.attention.self              | BertSelfAttention | 1 M   \n",
      "114 | model.encoder.layer.6.attention.self.query        | Linear            | 590 K \n",
      "115 | model.encoder.layer.6.attention.self.key          | Linear            | 590 K \n",
      "116 | model.encoder.layer.6.attention.self.value        | Linear            | 590 K \n",
      "117 | model.encoder.layer.6.attention.self.dropout      | Dropout           | 0     \n",
      "118 | model.encoder.layer.6.attention.output            | BertSelfOutput    | 592 K \n",
      "119 | model.encoder.layer.6.attention.output.dense      | Linear            | 590 K \n",
      "120 | model.encoder.layer.6.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "121 | model.encoder.layer.6.attention.output.dropout    | Dropout           | 0     \n",
      "122 | model.encoder.layer.6.intermediate                | BertIntermediate  | 2 M   \n",
      "123 | model.encoder.layer.6.intermediate.dense          | Linear            | 2 M   \n",
      "124 | model.encoder.layer.6.output                      | BertOutput        | 2 M   \n",
      "125 | model.encoder.layer.6.output.dense                | Linear            | 2 M   \n",
      "126 | model.encoder.layer.6.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "127 | model.encoder.layer.6.output.dropout              | Dropout           | 0     \n",
      "128 | model.encoder.layer.7                             | BertLayer         | 7 M   \n",
      "129 | model.encoder.layer.7.attention                   | BertAttention     | 2 M   \n",
      "130 | model.encoder.layer.7.attention.self              | BertSelfAttention | 1 M   \n",
      "131 | model.encoder.layer.7.attention.self.query        | Linear            | 590 K \n",
      "132 | model.encoder.layer.7.attention.self.key          | Linear            | 590 K \n",
      "133 | model.encoder.layer.7.attention.self.value        | Linear            | 590 K \n",
      "134 | model.encoder.layer.7.attention.self.dropout      | Dropout           | 0     \n",
      "135 | model.encoder.layer.7.attention.output            | BertSelfOutput    | 592 K \n",
      "136 | model.encoder.layer.7.attention.output.dense      | Linear            | 590 K \n",
      "137 | model.encoder.layer.7.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "138 | model.encoder.layer.7.attention.output.dropout    | Dropout           | 0     \n",
      "139 | model.encoder.layer.7.intermediate                | BertIntermediate  | 2 M   \n",
      "140 | model.encoder.layer.7.intermediate.dense          | Linear            | 2 M   \n",
      "141 | model.encoder.layer.7.output                      | BertOutput        | 2 M   \n",
      "142 | model.encoder.layer.7.output.dense                | Linear            | 2 M   \n",
      "143 | model.encoder.layer.7.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "144 | model.encoder.layer.7.output.dropout              | Dropout           | 0     \n",
      "145 | model.encoder.layer.8                             | BertLayer         | 7 M   \n",
      "146 | model.encoder.layer.8.attention                   | BertAttention     | 2 M   \n",
      "147 | model.encoder.layer.8.attention.self              | BertSelfAttention | 1 M   \n",
      "148 | model.encoder.layer.8.attention.self.query        | Linear            | 590 K \n",
      "149 | model.encoder.layer.8.attention.self.key          | Linear            | 590 K \n",
      "150 | model.encoder.layer.8.attention.self.value        | Linear            | 590 K \n",
      "151 | model.encoder.layer.8.attention.self.dropout      | Dropout           | 0     \n",
      "152 | model.encoder.layer.8.attention.output            | BertSelfOutput    | 592 K \n",
      "153 | model.encoder.layer.8.attention.output.dense      | Linear            | 590 K \n",
      "154 | model.encoder.layer.8.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "155 | model.encoder.layer.8.attention.output.dropout    | Dropout           | 0     \n",
      "156 | model.encoder.layer.8.intermediate                | BertIntermediate  | 2 M   \n",
      "157 | model.encoder.layer.8.intermediate.dense          | Linear            | 2 M   \n",
      "158 | model.encoder.layer.8.output                      | BertOutput        | 2 M   \n",
      "159 | model.encoder.layer.8.output.dense                | Linear            | 2 M   \n",
      "160 | model.encoder.layer.8.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "161 | model.encoder.layer.8.output.dropout              | Dropout           | 0     \n",
      "162 | model.encoder.layer.9                             | BertLayer         | 7 M   \n",
      "163 | model.encoder.layer.9.attention                   | BertAttention     | 2 M   \n",
      "164 | model.encoder.layer.9.attention.self              | BertSelfAttention | 1 M   \n",
      "165 | model.encoder.layer.9.attention.self.query        | Linear            | 590 K \n",
      "166 | model.encoder.layer.9.attention.self.key          | Linear            | 590 K \n",
      "167 | model.encoder.layer.9.attention.self.value        | Linear            | 590 K \n",
      "168 | model.encoder.layer.9.attention.self.dropout      | Dropout           | 0     \n",
      "169 | model.encoder.layer.9.attention.output            | BertSelfOutput    | 592 K \n",
      "170 | model.encoder.layer.9.attention.output.dense      | Linear            | 590 K \n",
      "171 | model.encoder.layer.9.attention.output.LayerNorm  | LayerNorm         | 1 K   \n",
      "172 | model.encoder.layer.9.attention.output.dropout    | Dropout           | 0     \n",
      "173 | model.encoder.layer.9.intermediate                | BertIntermediate  | 2 M   \n",
      "174 | model.encoder.layer.9.intermediate.dense          | Linear            | 2 M   \n",
      "175 | model.encoder.layer.9.output                      | BertOutput        | 2 M   \n",
      "176 | model.encoder.layer.9.output.dense                | Linear            | 2 M   \n",
      "177 | model.encoder.layer.9.output.LayerNorm            | LayerNorm         | 1 K   \n",
      "178 | model.encoder.layer.9.output.dropout              | Dropout           | 0     \n",
      "179 | model.encoder.layer.10                            | BertLayer         | 7 M   \n",
      "180 | model.encoder.layer.10.attention                  | BertAttention     | 2 M   \n",
      "181 | model.encoder.layer.10.attention.self             | BertSelfAttention | 1 M   \n",
      "182 | model.encoder.layer.10.attention.self.query       | Linear            | 590 K \n",
      "183 | model.encoder.layer.10.attention.self.key         | Linear            | 590 K \n",
      "184 | model.encoder.layer.10.attention.self.value       | Linear            | 590 K \n",
      "185 | model.encoder.layer.10.attention.self.dropout     | Dropout           | 0     \n",
      "186 | model.encoder.layer.10.attention.output           | BertSelfOutput    | 592 K \n",
      "187 | model.encoder.layer.10.attention.output.dense     | Linear            | 590 K \n",
      "188 | model.encoder.layer.10.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "189 | model.encoder.layer.10.attention.output.dropout   | Dropout           | 0     \n",
      "190 | model.encoder.layer.10.intermediate               | BertIntermediate  | 2 M   \n",
      "191 | model.encoder.layer.10.intermediate.dense         | Linear            | 2 M   \n",
      "192 | model.encoder.layer.10.output                     | BertOutput        | 2 M   \n",
      "193 | model.encoder.layer.10.output.dense               | Linear            | 2 M   \n",
      "194 | model.encoder.layer.10.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "195 | model.encoder.layer.10.output.dropout             | Dropout           | 0     \n",
      "196 | model.encoder.layer.11                            | BertLayer         | 7 M   \n",
      "197 | model.encoder.layer.11.attention                  | BertAttention     | 2 M   \n",
      "198 | model.encoder.layer.11.attention.self             | BertSelfAttention | 1 M   \n",
      "199 | model.encoder.layer.11.attention.self.query       | Linear            | 590 K \n",
      "200 | model.encoder.layer.11.attention.self.key         | Linear            | 590 K \n",
      "201 | model.encoder.layer.11.attention.self.value       | Linear            | 590 K \n",
      "202 | model.encoder.layer.11.attention.self.dropout     | Dropout           | 0     \n",
      "203 | model.encoder.layer.11.attention.output           | BertSelfOutput    | 592 K \n",
      "204 | model.encoder.layer.11.attention.output.dense     | Linear            | 590 K \n",
      "205 | model.encoder.layer.11.attention.output.LayerNorm | LayerNorm         | 1 K   \n",
      "206 | model.encoder.layer.11.attention.output.dropout   | Dropout           | 0     \n",
      "207 | model.encoder.layer.11.intermediate               | BertIntermediate  | 2 M   \n",
      "208 | model.encoder.layer.11.intermediate.dense         | Linear            | 2 M   \n",
      "209 | model.encoder.layer.11.output                     | BertOutput        | 2 M   \n",
      "210 | model.encoder.layer.11.output.dense               | Linear            | 2 M   \n",
      "211 | model.encoder.layer.11.output.LayerNorm           | LayerNorm         | 1 K   \n",
      "212 | model.encoder.layer.11.output.dropout             | Dropout           | 0     \n",
      "213 | model.pooler                                      | BertPooler        | 590 K \n",
      "214 | model.pooler.dense                                | Linear            | 590 K \n",
      "215 | model.pooler.activation                           | Tanh              | 0     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shauli/miniconda3/envs/py3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.9093616604804993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shauli/miniconda3/envs/py3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99fde763d934181994546a46c1d0e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-19 21:45:02.731 INFO    lightning: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.to(\"cpu\")\n",
    "#model.device = \"cpu\"\n",
    "trainer = Trainer(max_nb_epochs=1,min_nb_epochs=1, gpus = 1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "sents_concat, ind_arg1, ind_arg2, bert_tok_arg1, bert_tok_arg2 = get_prediction(data[k][\"first\"], data[k][\"second\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genes\n",
      "HEK\n",
      "genes\n",
      "hek\n"
     ]
    }
   ],
   "source": [
    "print(sents_concat.split(\" \")[ind_arg1])\n",
    "print(sents_concat.split(\" \")[ind_arg2])\n",
    "\n",
    "print(bert_tok_arg1)\n",
    "print(bert_tok_arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For example , Fornek et al. showed that the H5N1 influenza ARG1:A/Hong Kong/486/97 virus containing amino acid substitution E627 K in PB2 upregulated TCR ARG2:complex genes ( e.g. , CD3D and CD3 G ) in the lungs of infected mice at day 2 post-infection [ 11 ] . ***** In a third approach , the adeno-associated virus rep protein mediates stable integration of antibody genes into HEK cells .'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
