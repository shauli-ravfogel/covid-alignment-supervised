{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import spike_queries\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertModel, AutoTokenizer, AutoModel, PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict, Tuple\n",
    "from scipy.spatial.distance import cosine as cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries.txt\", \"r\") as f:\n",
    "    queries = f.readlines()\n",
    "    queries = [l.strip() for l in queries]\n",
    "    \n",
    "queries = queries[:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"covid19\"\n",
    "# num_results = 100\n",
    "# query_type = \"syntactic\"\n",
    "\n",
    "# for q in tqdm.tqdm(queries, total = len(queries)):\n",
    "#     df = spike_queries.perform_query(q, dataset_name, num_results, query_type) #previously: word=Hawaii\n",
    "#     queries2results[q] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"queries2results.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(queries2results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries2results.pickle\", \"rb\") as f:\n",
    "    queries2results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for q in queries:\n",
    "    #print(q)\n",
    "    results = queries2results[q]\n",
    "    sents, arg1_first, arg2_first = results[\"sentence_text\"].tolist(), results[\"arg1_first_index\"].tolist(), results[\"arg2_first_index\"].tolist()\n",
    "    arg1_last, arg2_last = results[\"arg1_last_index\"].tolist(), results[\"arg2_last_index\"].tolist()\n",
    "    \n",
    "    sents_with_args = []\n",
    "    for s,arg1_ind,arg2_ind, arg1_ind_last, arg2_ind_last in zip(sents,arg1_first,arg2_first, arg1_last, arg2_last):\n",
    "        s_lst = s.split(\" \")\n",
    "        if arg1_ind > arg2_ind:\n",
    "            arg1_ind, arg2_ind = arg2_ind, arg1_ind\n",
    "            arg1_ind_last, arg2_ind_last = arg2_ind_last, arg1_ind_last\n",
    "            arg1_str, arg2_str = \"ARG2:\", \"ARG1:\"\n",
    "        else:\n",
    "            arg1_str, arg2_str = \"ARG1:\", \"ARG2:\"\n",
    "        s_with_args = s_lst[:arg1_ind] + [arg1_str+s_lst[arg1_ind]] + s_lst[arg1_ind+1:arg2_ind] + [arg2_str+s_lst[arg2_ind]] + s_lst[arg2_ind+1:]\n",
    "        s_with_args = \" \".join(s_with_args)\n",
    "        sents_with_args.append({\"sent\": s_with_args, \"start_1\": arg1_ind, \"start_2\": arg2_ind, \"end_1\": arg1_ind_last,\n",
    "                               \"end_2\": arg2_ind_last})\n",
    "    \n",
    "    max_number_of_pairs = 500\n",
    "    pairs = list(itertools.product(sents_with_args, repeat=2))\n",
    "    print(len(pairs))\n",
    "    random.shuffle(pairs)\n",
    "    for pair in pairs[:max_number_of_pairs]:\n",
    "        data.append({\"first\": pair[0][\"sent\"], \"second\": pair[1][\"sent\"], \"query\": q, \"first_arg1\": (pair[0][\"start_1\"], pair[0][\"end_1\"]),\n",
    "                    \"first_arg2\": (pair[0][\"start_2\"], pair[0][\"end_2\"]), \"second_arg1\": (pair[1][\"start_1\"], pair[1][\"end_1\"]),\n",
    "                   \"second_arg2\": (pair[1][\"start_2\"], pair[1][\"end_2\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "data[0]\n",
    "\n",
    "with open(\"data.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    for d in data:\n",
    "        first, second = d[\"first\"], d[\"second\"]\n",
    "        first_arg1 = d[\"first_arg1\"]\n",
    "        first_arg2 = d[\"first_arg2\"]\n",
    "        second_arg1 = d[\"second_arg1\"]\n",
    "        second_arg2 = d[\"second_arg2\"]\n",
    "        \n",
    "        elems = [first, second, first_arg1, first_arg2, second_arg1, second_arg2]\n",
    "        keys = [\"first\", \"second\", \"first_arg1\", \"first_arg2\"]\n",
    "        \n",
    "        f.write(json.dumps(d) + \"\\n\")\n",
    "        #f.write(d[\"first\"] + \"\\t\" + d[\"second\"] + \"\\t\" + d[\"query\"] + \"\\t\" + \"-\".join(d[\"first_arg1\"])+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries2results[queries[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global count\n",
    "count = 0\n",
    "global total\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, train_dataset: Dataset, dev_dataset: Dataset, batch_size, device: str, mode: str = \"eval\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased', output_hidden_states=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=config)    \n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.linear_arg1_1 = torch.nn.Linear(768, 256)\n",
    "        self.linear_arg2_1 = self.linear_arg1_1 #torch.nn.Linear(768, 100)\n",
    "        self.linear_arg1_2 = self.linear_arg1_1 #torch.nn.Linear(768, 50)\n",
    "        self.linear_arg2_2 = self.linear_arg2_1 #torch.nn.Linear(768, 50)\n",
    "        \n",
    "        if mode == \"eval\":\n",
    "            \n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.model.encoder.layer[-1].parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        self.linear_arg1_1.requires_grad = True\n",
    "        self.linear_arg2_1.requires_grad = True\n",
    "        self.linear_arg1_2.requires_grad = True\n",
    "        self.linear_arg2_2.requires_grad = True\n",
    "        \n",
    "    \n",
    "        self.train_gen = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "        self.dev_gen = torch.utils.data.DataLoader(self.dev_dataset, batch_size=batch_size, drop_last=False, shuffle=False)\n",
    "        self.acc = None\n",
    "\n",
    "        \n",
    "    def tokenize(self, original_sentence: List[str]) -> Tuple[List[str], Dict[int, int]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        bert_tokens: The sentence, tokenized by BERT tokenizer.\n",
    "        orig_to_tok_map: An output dictionary consisting of a mapping (alignment) between indices in the original tokenized sentence, and indices in the sentence tokenized by the BERT tokenizer. See https://github.com/google-research/bert\n",
    "        \"\"\"\n",
    "\n",
    "        bert_tokens = [\"[CLS]\"]\n",
    "        orig_to_tok_map = {}\n",
    "        tok_to_orig_map = {}\n",
    "        has_subwords = False\n",
    "        is_subword = []\n",
    "\n",
    "        for i, w in enumerate(original_sentence):\n",
    "            tokenized_w = self.tokenizer.tokenize(w)\n",
    "            has_subwords = len(tokenized_w) > 1\n",
    "            is_subword.append(has_subwords)\n",
    "            bert_tokens.extend(tokenized_w)\n",
    "\n",
    "            orig_to_tok_map[i] = len(bert_tokens) - 1\n",
    "\n",
    "        tok_to_orig_map = {}\n",
    "\n",
    "        bert_tokens.append(\"[SEP]\")\n",
    "        tok_to_orig_map = get_tok_to_orig_map(orig_to_tok_map, len(original_sentence), len(bert_tokens))        \n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)\n",
    "\n",
    "        return (bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        states = outputs[0][0] #[seq_len, 768]\n",
    "        return states\n",
    "    \n",
    "    def forward_with_loss_calculation2(self, bert_tokens, x, range_sent1, range_sent2, orig_to_tok_map, l, l_tokens):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        states = outputs[0][0] #[seq_len, 768]\n",
    "        states = states / torch.norm(states, dim = 1, keepdim = True)\n",
    "        \n",
    "        arg1_sent1, arg2_sent1 = range_sent1\n",
    "        arg1_sent2, arg2_sent2 = range_sent2\n",
    "        \n",
    "        #print(range_sent1)\n",
    "        #print(arg1_sent1, arg2_sent1)\n",
    "        #print(range_sent2)\n",
    "        #print(arg1_sent2, arg2_sent2)\n",
    "        #print(states[arg1_sent1[0]:arg1_sent1[1]].shape)\n",
    "        print(\"sent1 args (gold):\", bert_tokens[arg1_sent1[0]+3:arg1_sent1[1]], bert_tokens[arg2_sent1[0]+3:arg2_sent1[1]])\n",
    "        print(\"sent2 args (gold):\", bert_tokens[arg1_sent2[0]:arg1_sent2[1]], bert_tokens[arg2_sent2[0]:arg2_sent2[1]])\n",
    "        print(\"------------------------------------------------\")\n",
    "        \n",
    "        sent1_arg1_vec, sent1_arg2_vec = states[arg1_sent1[0]+3:arg1_sent1[1]].mean(dim=0), states[arg2_sent1[0]+3:arg2_sent1[1]].mean(dim=0)\n",
    "        sent2_arg1_vec, sent2_arg2_vec = states[arg1_sent2[0]:arg1_sent2[1]].mean(dim=0), states[arg2_sent2[0]:arg2_sent2[1]].mean(dim=0)\n",
    "            \n",
    "        dists_arg1 = ((self.linear_arg1_1(states)-self.linear_arg1_2(sent1_arg1_vec))**2).sum(dim = 1).detach().cpu().numpy()\n",
    "        dists_arg2 = ((self.linear_arg2_1(states)-self.linear_arg2_2(sent1_arg2_vec)**2)).sum(dim = 1).detach().cpu().numpy()       \n",
    "        #states_projected = self.linear_arg1_1(states)\n",
    "        #arg1_sent1_projected = self.linear_arg1_2(sent1_arg1_vec)\n",
    "        #arg2_sent1_projected = self.linear_arg2_2(sent1_arg2_vec)\n",
    "        #dists_arg1 = cosine_distance(arg1_sent1_projected, states_projected)\n",
    "        #dists_arg2 = cosine_distance(arg2_sent1_projected, states_projected)\n",
    "        \n",
    "        \n",
    "        mask_gold_arg1 = np.zeros_like(dists_arg1)\n",
    "        mask_gold_arg2 = np.zeros_like(dists_arg2)\n",
    "        mask_gold_arg1[arg1_sent1[0]:arg1_sent1[1]] = 1e6\n",
    "        mask_gold_arg1[arg1_sent2[0]:arg1_sent2[1]] = 1e6\n",
    "        mask_gold_arg2[arg2_sent1[0]:arg2_sent1[1]] = 1e6\n",
    "        mask_gold_arg2[arg2_sent2[0]:arg2_sent2[1]] = 1e6\n",
    "        mask_gold_arg2[:l_tokens] = 1e6\n",
    "        mask_gold_arg1[:l_tokens] = 1e6\n",
    "\n",
    "        \n",
    "        idx_arg1 = np.argsort(dists_arg1+mask_gold_arg1)\n",
    "        idx_arg2 = np.argsort(dists_arg2+mask_gold_arg2)\n",
    "        k = 0 if np.random.random() < 0.5 else random.choice(range(5))\n",
    "        dist_arg1_argmax = (self.linear_arg1_2(sent1_arg1_vec) - self.linear_arg1_1(states[idx_arg1[k]])).norm()#**2\n",
    "        dist_arg2_argmax = (self.linear_arg2_2(sent1_arg2_vec) - self.linear_arg2_1(states[idx_arg2[k]])).norm()#**2\n",
    "        dist_arg1_gold = (self.linear_arg1_2(sent1_arg1_vec) - self.linear_arg1_1(sent2_arg1_vec)).norm()#**2\n",
    "        dist_arg2_gold = (self.linear_arg2_2(sent1_arg2_vec) - self.linear_arg2_1(sent2_arg2_vec)).norm()#**2\n",
    "        \n",
    "        print(\"arg1-argmax, arg1-gold: {}, {}\".format(dist_arg1_argmax, dist_arg1_gold))\n",
    "        print(\"arg2-argmax, arg2-gold: {}, {}\".format(dist_arg2_argmax, dist_arg2_gold))\n",
    "        \n",
    "        # softmax triplet\n",
    "        \n",
    "        z = torch.max(dist_arg1_argmax, dist_arg1_gold)\n",
    "        temp = 2\n",
    "        pos_arg1 = torch.exp((dist_arg1_gold - z)/temp)\n",
    "        neg_arg1 = torch.exp((dist_arg1_argmax - z)/temp)\n",
    "        loss_arg1 = (pos_arg1 / (pos_arg1 + neg_arg1))**2\n",
    "\n",
    "        z = torch.max(dist_arg2_argmax, dist_arg2_gold)\n",
    "        pos_arg2 = torch.exp((dist_arg2_gold - z)/temp)\n",
    "        neg_arg2 = torch.exp((dist_arg2_argmax - z)/temp)\n",
    "        loss_arg2 = (pos_arg2 / (pos_arg2 + neg_arg2))**2\n",
    "\n",
    "        alpha = 0.01\n",
    "        loss_arg1 = torch.max(torch.zeros(1).to(self.device), dist_arg1_gold - dist_arg1_argmax + alpha)\n",
    "        loss_arg2 = torch.max(torch.zeros(1).to(self.device), dist_arg2_gold - dist_arg2_argmax + alpha)\n",
    "        \n",
    "        global count\n",
    "        global total\n",
    "        if pos_arg1 < neg_arg1 and pos_arg2 < neg_arg2:\n",
    "            count += 1\n",
    "        total += 1                                                      \n",
    "        # check            \n",
    "        loss = loss_arg1 + loss_arg2\n",
    "        return loss, np.argsort(dists_arg1+mask_gold_arg1)\n",
    "    \n",
    "    def forward_with_loss_calculation(self, x, sent1_idx, sent2_idx, orig_to_tok_map, tok_to_orig_map, l, l_tokens):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        states = outputs[0][0] #[seq_len, 768]\n",
    "        #states = states/torch.norm(states, dim = 1, keepdim = True)\n",
    "        \n",
    "        sent1_idx_arg1 = sent1_idx[0].detach().cpu().numpy().item()\n",
    "        sent1_idx_arg2 = sent1_idx[1].detach().cpu().numpy().item()\n",
    "        gold_arg1_sent1 = orig_to_tok_map[sent1_idx_arg1]\n",
    "        gold_arg2_sent1 = orig_to_tok_map[sent1_idx_arg2]\n",
    "        \n",
    "        sent1_arg1_vec, sent1_arg2_vec = states[gold_arg1_sent1], states[gold_arg2_sent1]\n",
    "        \n",
    "        states_sent2 = states[orig_to_tok_map[l.detach().cpu().numpy().item()]:]  \n",
    "        \n",
    "        sims_arg1 = sent1_arg1_vec @ states.T\n",
    "        sims_arg2 = sent1_arg2_vec @ states.T\n",
    "        \n",
    "        sent2_idx_arg1 = sent2_idx[0].detach().cpu().numpy().item()\n",
    "        sent2_idx_arg2 = sent2_idx[1].detach().cpu().numpy().item()\n",
    "        gold_arg1_sent2, gold_arg2_sent2 = orig_to_tok_map[sent2_idx_arg1], orig_to_tok_map[sent2_idx_arg2]\n",
    "\n",
    "        dists_arg1 = ((self.linear_arg1(states-sent1_arg1_vec))**2).sum(dim = 1).detach().cpu().numpy()\n",
    "        dists_arg2 = ((self.linear_arg2(states-sent1_arg2_vec)**2)).sum(dim = 1).detach().cpu().numpy()\n",
    "               \n",
    "        mask_gold_arg1 = np.zeros_like(dists_arg1)\n",
    "        mask_gold_arg2 = np.zeros_like(dists_arg2)\n",
    "        mask_gold_arg1[gold_arg1_sent2] = 1e6\n",
    "        mask_gold_arg2[gold_arg2_sent2] = 1e6\n",
    "        mask_gold_arg2[:l_tokens] = 1e6\n",
    "        mask_gold_arg1[:l_tokens] = 1e6\n",
    "        \n",
    "        if np.random.random() < 0.0*1e-1:\n",
    "            print(dists_arg1+mask_gold_arg2)\n",
    "            print(\"=========================\")\n",
    "        \n",
    "        idx_arg1 = np.argsort(dists_arg1+mask_gold_arg1)\n",
    "        idx_arg2 = np.argsort(dists_arg2+mask_gold_arg2)\n",
    "        \n",
    "        k = 0 #random.choice(range(0,7))\n",
    "        #idx_arg1 = [orig_to_tok_map[i] for i in orig_to_tok_map.keys() if i != sent2_idx_arg1 and i > l]\n",
    "        #idx_arg2 = [orig_to_tok_map[i] for i in orig_to_tok_map.keys() if i != sent2_idx_arg2 and i > l]\n",
    "        #random.shuffle(idx_arg1)\n",
    "        #random.shuffle(idx_arg2)\n",
    "        \n",
    "        dist_arg1_argmax = (self.linear_arg1(sent1_arg1_vec - states[idx_arg1[k]])).norm()#**2\n",
    "        dist_arg2_argmax = (self.linear_arg2(sent1_arg2_vec - states[idx_arg2[k]])).norm()#**2\n",
    "        dist_arg1_gold = (self.linear_arg1(sent1_arg1_vec - states[gold_arg1_sent2])).norm()#**2\n",
    "        dist_arg2_gold = (self.linear_arg2(sent1_arg2_vec - states[gold_arg2_sent2])).norm()#**2\n",
    "        \n",
    "        \n",
    "        #alpha = 0.01\n",
    "        #loss_arg1 = torch.max(torch.zeros(1).to(self.device), -sim_arg1_gold + most_sim_arg1 + alpha)\n",
    "        #loss_arg2 = torch.max(torch.zeros(1).to(self.device), -sim_arg2_gold + most_sim_arg2 + alpha)\n",
    "        alpha = 1\n",
    "        loss_arg1 = torch.max(torch.zeros(1).to(self.device), dist_arg1_gold - dist_arg1_argmax + alpha)\n",
    "        loss_arg2 = torch.max(torch.zeros(1).to(self.device), dist_arg2_gold - dist_arg2_argmax + alpha)\n",
    "\n",
    "        # softmax triplet\n",
    "        \n",
    "        z = torch.max(dist_arg1_argmax, dist_arg1_gold)\n",
    "        temp = 1\n",
    "        pos_arg1 = torch.exp((dist_arg1_gold - z)/temp)\n",
    "        neg_arg1 = torch.exp((dist_arg1_argmax - z)/temp)\n",
    "        loss_arg1 = (pos_arg1 / (pos_arg1 + neg_arg1))**2\n",
    "\n",
    "        z = torch.max(dist_arg2_argmax, dist_arg2_gold)\n",
    "        pos_arg2 = torch.exp((dist_arg2_gold - z)/temp)\n",
    "        neg_arg2 = torch.exp((dist_arg2_argmax - z)/temp)\n",
    "        loss_arg2 = (pos_arg2 / (pos_arg2 + neg_arg2))**2\n",
    "                                                               \n",
    "        # check\n",
    "        global count\n",
    "        \n",
    "        if tok_to_orig_map[idx_arg1[0]] == sent2_idx_arg1:\n",
    "            print(\"yay\",)\n",
    "            count += 1\n",
    "            loss_arg1 = torch.zeros(1).to(self.device).squeeze()\n",
    "            \n",
    "        if tok_to_orig_map[idx_arg2[0]] == sent2_idx_arg2:\n",
    "            print(\"yay2\")\n",
    "            count += 1\n",
    "            loss_arg2 = torch.zeros(1).to(self.device).squeeze()                                                      \n",
    "            \n",
    "        loss = loss_arg1 + loss_arg2\n",
    "        return loss, np.argsort(dists_arg1+mask_gold_arg1)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        sents_concat, sent1_idx, sent2_idx, l = batch\n",
    "        sent1_idx = [x.detach().cpu().numpy().item() for x in sent1_idx]\n",
    "        sent2_idx = [x.detach().cpu().numpy().item() for x in sent2_idx]\n",
    "        \n",
    "        #print(sents_concat)\n",
    "        #print(\"---------------------------------------------\")\n",
    "        bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = self.tokenize(sents_concat[0].split(\" \"))        \n",
    "        l_tokens = len(bert_tokens[:orig_to_tok_map[l.detach().cpu().numpy().item()-1]]) \n",
    "        sent1_range_arg1 = get_entity_range(sent1_idx[0], orig_to_tok_map)\n",
    "        sent1_range_arg2 = get_entity_range(sent1_idx[1], orig_to_tok_map)\n",
    "        sent2_range_arg1 = get_entity_range(sent2_idx[0], orig_to_tok_map)\n",
    "        sent2_range_arg2 = get_entity_range(sent2_idx[1], orig_to_tok_map)\n",
    "        range_sent1 = (sent1_range_arg1,sent1_range_arg2)\n",
    "        range_sent2 = (sent2_range_arg1,sent2_range_arg2)\n",
    "        \n",
    "        loss, argsort = self.forward_with_loss_calculation2(bert_tokens, tokens_tensor, range_sent1, range_sent2, orig_to_tok_map, l, l_tokens)\n",
    "        #loss, argsort = self.forward_with_loss_calculation(tokens_tensor, sent1_idx, sent2_idx, orig_to_tok_map,tok_to_orig_map, l, l_tokens)\n",
    "        print(sents_concat)\n",
    "        print(argsort[:5])\n",
    "        print(\"preds\", bert_tokens[argsort[0]], bert_tokens[argsort[1]], bert_tokens[argsort[2]])\n",
    "        print(\"gold\", bert_tokens[orig_to_tok_map[sent2_idx[0]]])\n",
    "        print(\"count\", count/total)\n",
    "        print(\"==================================\")\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "\n",
    "        sents_concat, sent1_idx, sent2_idx, l = batch\n",
    "        print(sent1_idx, type(sent1_idx))\n",
    "        \n",
    "        sent1_idx = [x.detach().cpu().numpy().item() for x in sent1_idx]\n",
    "        sent2_idx = [x.detach().cpu().numpy().item() for x in sent2_idx]\n",
    "        \n",
    "        bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = self.tokenize(sents_concat[0].split(\" \"))\n",
    "        l_tokens = len(bert_tokens[:orig_to_tok_map[l.detach().cpu().numpy().item()-1]]) \n",
    "        sent1_range_arg1 = get_entity_range(sent1_idx[0], orig_to_tok_map)\n",
    "        sent1_range_arg2 = get_entity_range(sent1_idx[1], orig_to_tok_map)\n",
    "        sent2_range_arg1 = get_entity_range(sent2_idx[0], orig_to_tok_map)\n",
    "        sent2_range_arg2 = get_entity_range(sent2_idx[1], orig_to_tok_map)\n",
    "        range_sent1 = (sent1_range_arg1,sent1_range_arg2)\n",
    "        range_sent2 = (sent2_range_arg1,sent2_range_arg2)\n",
    "        loss, argsort = self.forward_with_loss_calculation2(bert_tokens, tokens_tensor, range_sent1, range_sent2, orig_to_tok_map, l, l_tokens)\n",
    "        #loss, argsort = self.forward_with_loss_calculation(tokens_tensor, sent1_idx, sent2_idx, orig_to_tok_map, tok_to_orig_map, l, l_tokens)\n",
    "\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        print(\"Loss is {}\".format(avg_loss))\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=0.0015, momentum=0.9, weight_decay=1e-5)\n",
    "        #return torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self.train_gen\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return self.dev_gen\n",
    "    \n",
    "\n",
    "def get_entity_range(index_orig, orig_to_tok_map):\n",
    "    \n",
    "    m = min(orig_to_tok_map.keys())\n",
    "    if orig_to_tok_map[index_orig] == 1: return (1,2)\n",
    "    if index_orig == 0: return (1, orig_to_tok_map[index_orig] + 1)\n",
    "    \n",
    "    before = index_orig - 1\n",
    "    tok_range = (orig_to_tok_map[before] + 1, orig_to_tok_map[index_orig] + 1)\n",
    "    return tok_range\n",
    "\n",
    "\n",
    "def get_tok_to_orig_map(orig_to_tok_map, num_words, num_tokens):\n",
    "    \n",
    "    ranges = [get_entity_range(i, orig_to_tok_map) for i in range(num_words)]\n",
    "    tok_to_orig_map = {}\n",
    "    for i in range(num_words):\n",
    "        min,max = ranges[i]\n",
    "        for tok in range(min,max):\n",
    "            tok_to_orig_map[tok] = i\n",
    "    \n",
    "    for tok in range(num_tokens):\n",
    "        if tok not in tok_to_orig_map:\n",
    "            tok_to_orig_map[tok] = num_words -1\n",
    "    \n",
    "    return tok_to_orig_map\n",
    "        \n",
    "\n",
    "def get_prediction(sent1, sent2, model):\n",
    "    \n",
    "    l = len(sent1.split(\" \")) + 1 \n",
    "    sent2 = sent2.replace(\"ARG1:\", \"\").replace(\"ARG2:\", \"\")\n",
    "    \n",
    "    sents_concat = sent1 + \" ***** \" + sent2\n",
    "    sent1_arg1 = [i for i,w in enumerate(sent1.split(\" \")) if \"ARG1:\" in w][0]\n",
    "    sent1_arg2 = [i for i,w in enumerate(sent1.split(\" \")) if \"ARG2:\" in w][0]\n",
    "    words = sents_concat.split(\" \")\n",
    "    \n",
    "    bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = model.tokenize(sents_concat.split(\" \"))\n",
    "    range_arg1 = get_entity_range(sent1_arg1, orig_to_tok_map)\n",
    "    range_arg2 = get_entity_range(sent1_arg2, orig_to_tok_map)\n",
    "    \n",
    "    l_tokens = len(bert_tokens[:orig_to_tok_map[l-1]])    \n",
    "    outputs = model.model(tokens_tensor)\n",
    "    states = outputs[0][0] #[seq_len, 768]\n",
    "\n",
    "    states = states/torch.norm(states, dim = 1, keepdim = True)\n",
    "    \n",
    "    sent1_arg1_vec, sent1_arg2_vec = states[range_arg1[0]:range_arg1[1]].mean(dim=0), states[range_arg2[0]:range_arg2[1]].mean(dim=0)\n",
    "    \n",
    "    print(\"GOLD ARG1, ARG2:\", bert_tokens[orig_to_tok_map[sent1_arg1]], bert_tokens[orig_to_tok_map[sent1_arg2]])\n",
    "    sims_arg1 = (model.linear_arg1_1(states)-model.linear_arg1_2(sent1_arg1_vec)).norm(dim=1)\n",
    "    sims_arg2 = (model.linear_arg2_1(states)-model.linear_arg2_2(sent1_arg2_vec)).norm(dim=1)\n",
    "\n",
    "    mask_gold_arg1 = torch.zeros_like(sims_arg1).to(model.device)\n",
    "    mask_gold_arg2 = torch.zeros_like(sims_arg2).to(model.device)\n",
    "    mask_gold_arg2[:l_tokens] = 1e6\n",
    "    mask_gold_arg1[:l_tokens] = 1e6\n",
    "        \n",
    "    idx_arg1 = torch.argsort(sims_arg1+mask_gold_arg1).detach().cpu().numpy()\n",
    "    idx_arg2 = torch.argsort(sims_arg2+mask_gold_arg2).detach().cpu().numpy()\n",
    "    \n",
    "    print(\"all arg1 preds\", [words[tok_to_orig_map[idx_arg1[i]]] for i in range(len(idx_arg1))][:7])\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"all arg2 preds\", [words[tok_to_orig_map[idx_arg2[i]]] for i in range(len(idx_arg2))][:7])\n",
    "    print(\"---------------------------------------------------\")\n",
    "    \n",
    "    print(\"sims\", sims_arg1[idx_arg1])\n",
    "    \n",
    "    print(\"==================================\")\n",
    "    print(states)\n",
    "    print(\"=================================\")\n",
    "    \n",
    "    #print(bert_tokens[idx_arg1[-4]], bert_tokens[idx_arg2[-4]])\n",
    "    \n",
    "    return\n",
    "    \n",
    "    most_sim_arg1, most_sim_arg2 = sims_arg1[idx_arg1[-2]], sims_arg2[idx_arg2[-2]]\n",
    "    ind_arg1, ind_arg2 = idx_arg1[-2].detach().cpu().numpy().item(), idx_arg2[-2].detach().cpu().numpy().item()\n",
    "    ind_arg1, ind_arg2 = ind_arg1 + l_tokens, ind_arg2 + l_tokens\n",
    "    \n",
    "    return sents_concat, tok_to_orig_map[ind_arg1], tok_to_orig_map[ind_arg2], bert_tokens[ind_arg1], bert_tokens[ind_arg2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple torch dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Dict], device = \"cpu\"):\n",
    "\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            d = self.data[index]\n",
    "            sent1, sent2 = d[\"first\"], d[\"second\"]\n",
    "            sent1_arg1, sent1_arg2 = d[\"first_arg1\"][0], d[\"first_arg2\"][0]\n",
    "            sent2_arg1, sent2_arg2 = d[\"second_arg1\"][0], d[\"second_arg2\"][0]\n",
    "            \n",
    "            l = len(sent1.split(\" \")) + 1 \n",
    "            sent2_arg1, sent2_arg2 = sent2_arg1 + l, sent2_arg2 + l\n",
    "\n",
    "            sent2 = sent2.replace(\"ARG1:\", \"\").replace(\"ARG2:\", \"\")\n",
    "            sents_concat = sent1 + \" ***** \" + sent2 #sents_concat.split(\" \")[l] is the first token in the 2nd sent\n",
    "            \n",
    "            return sents_concat, (sent1_arg1, sent1_arg2), (sent2_arg1, sent2_arg2), l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = int(0.8 * len(data))\n",
    "train_dataset, dev_dataset = Dataset(data[:l], \"cpu\"), Dataset(data[l:], \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_concat, (sent1_arg1, sent1_arg2), (sent2_arg1, sent2_arg2), l = train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel(train_dataset, dev_dataset, 1, \"cuda\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"it is very important.\"\n",
    "bert_tokens, orig_to_tok_map, tok_to_orig_map, tokens_tensor = model.tokenize(sent.split(\" \"))\n",
    "indexed_tokens = model.tokenizer.convert_tokens_to_ids(bert_tokens)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_to_tok_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_to_orig_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(\"cpu\")\n",
    "#model.device = \"cpu\"\n",
    "trainer = Trainer(max_nb_epochs=10,min_nb_epochs=1, gpus = 1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = random.choice(range(100))\n",
    "\n",
    "sent1 = \"The use of 5 % ARG1:dextrose and sodium bicarbonate is an effective initial treatment for ARG2:hyperkalemia .\"\n",
    "sent2 = \"Cyclosporine is an immunosuppressant and is used to avoid organ transplant rejection .\"\n",
    "\n",
    "# print(data[k][\"first\"])\n",
    "# print(\"===============================\")\n",
    "# print(data[k][\"second\"])\n",
    "# print(\"===============================\")\n",
    "\n",
    "#get_prediction(data[k][\"first\"], data[k][\"second\"], model.to(model.device))\n",
    "with torch.no_grad():\n",
    "    get_prediction(sent1, sent2, model.eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
